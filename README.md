# AI Healthcare Data Agent (POC)

An intelligent healthcare data analysis system that combines natural language processing with interactive data visualizations. Ask questions in plain English and get instant insights with beautiful Plotly.js charts.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![Node](https://img.shields.io/badge/node-23+-green.svg)
![Docker](https://img.shields.io/badge/docker-ready-blue.svg)

## üöÄ Quick Start (Docker - Recommended)

**The fastest way to get started:**

```powershell
# Windows
.\docker-start.ps1

# Linux/Mac
chmod +x docker-start.sh
./docker-start.sh
```

This will:
- ‚úÖ Start Ollama (local LLM - Qwen2.5:3b)
- ‚úÖ Start FastAPI backend
- ‚úÖ Start React frontend with Nginx
- ‚úÖ Pull the AI model (~2GB)
- ‚úÖ Open at http://localhost:3000

**Requirements:** Docker Desktop ([Download](https://www.docker.com/products/docker-desktop))

**See full Docker guide:** [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md)

---

## üìã Table of Contents

- [Features](#features)
- [Visual Insights](#-visual-insights)
- [Tech Stack](#tech-stack)
- [Quick Start (Docker)](#-quick-start-docker---recommended)
- [Manual Installation](#manual-installation)
  - [Ubuntu/Linux](#ubuntulinux)
  - [Windows](#windows)
- [Configuration](#configuration)
- [Usage](#usage)
- [Docker Deployment](#docker-deployment)
- [Local Model Setup](#local-model-setup-ollama)
- [Project Structure](#project-structure)
- [Supported Visualizations](#supported-visualizations)
- [Troubleshooting](#troubleshooting)
- [Development](#development)
- [Security](#security)

---

## Features

- ü§ñ **Natural Language Queries**: Ask questions in plain English about healthcare data
- üìä **60+ Interactive Visualizations**: Powered by Plotly.js with real-time chart type switching
- üó∫Ô∏è **Geographic Mapping**: Choropleth maps for state-level data analysis
- üí¨ **Persistent Chat History**: SQLite-backed conversation storage with configurable retention
- üé® **Futuristic HUD Interface**: Cyberpunk-inspired dark theme with glassmorphism effects
- üîÑ **Smart Visualization Selection**: AI-powered chart type recommendation based on data structure
- üéØ **Hybrid LLM Support**: Local models (Ollama) or Cloud (Google Gemini)
- üê≥ **Docker Ready**: One-command deployment with Docker Compose

## ‚ú® Visual Insights

| Choropleth Map | Pie Chart | Bar Chart |
|:---:|:---:|:---:|
| ![Choropleth Map](docs/images/choropleth_map.png) | ![Pie Chart](docs/images/pie_chart.png) | ![Bar Chart](docs/images/bar_chart.png) |
| *Patient count by state* | *Gender distribution* | *Average age by state* |

## Tech Stack

### Frontend
- **React 19** with TypeScript
- **Vite** for blazing-fast development
- **Plotly.js** for interactive visualizations (60+ chart types)
- **Tailwind CSS** for styling
- **Nginx** for production serving

### Backend
- **FastAPI** (Python) for REST API
- **SQLite** for data storage and chat history
- **Google Gemini AI** or **Ollama** (local) for natural language processing
- **Pandas** for data manipulation
- **Uvicorn** ASGI server

### Infrastructure
- **Docker & Docker Compose** for containerization
- **Ollama** for local LLM inference (Qwen2.5:3b)

---

## Docker Deployment

### Quick Start

```bash
# Copy environment file
cp .env.docker .env

# Start all services
docker-compose up -d

# Pull Ollama model
docker exec -it mediquery-ai-ollama ollama pull qwen2.5:3b

# Access the application
# Frontend: http://localhost:3000
# Backend:  http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Services

| Service | Port | Description |
|---------|------|-------------|
| **Frontend** | 3000 | React + Nginx |
| **Backend** | 8000 | FastAPI + Python |
| **Ollama** | 11434 | Local LLM (Qwen2.5:3b) |

**Full Docker guide:** [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md)

---

## Manual Installation

### Prerequisites

- **Python 3.9+**
- **Node.js 23+** and npm
- **Google Gemini API Key** ([Get one here](https://makersuite.google.com/app/apikey)) - Optional if using local model

### Ubuntu/Linux

#### 1. Clone the Repository
```bash
git clone https://github.com/elis-salobehaj/mediqueryAI.git
cd mediqueryAI

# Create .env file at PROJECT ROOT
cp .env.example .env
vi .env  # Add your API key or set USE_LOCAL_MODEL=true
```

#### 2. Backend Setup

```bash
cd backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Frontend Setup

```bash
cd ../frontend
npm install --legacy-peer-deps
```

#### 4. Running the Application

**Terminal 1 - Backend:**
```bash
cd backend
source venv/bin/activate
uvicorn main:app --reload
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
```

Access at: http://localhost:5173

### Windows

```powershell
# Backend
cd backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your settings

# Frontend (new terminal)
cd frontend
npm install --legacy-peer-deps

# Run backend
cd backend
venv\Scripts\activate
uvicorn main:app --reload

# Run frontend (new terminal)
cd frontend
npm run dev
```

---

## Configuration

### Environment Variables

Edit `backend/.env` or `.env` (for Docker):

```bash
# Google Gemini API Key (optional if using local model)
GEMINI_API_KEY=your_api_key_here

# Anthropic API Key (optional if using local model)
ANTHROPIC_API_KEY=your_api_key_here

# Chat History Configuration
CHAT_HISTORY_RETENTION_HOURS=24

# Local Model Configuration (Ollama)
USE_LOCAL_MODEL=true              # true = local, false = cloud
LOCAL_MODEL_NAME=qwen2.5:3b       # Ollama model
OLLAMA_HOST=http://localhost:11434
```

### Available Models

**Local (Ollama):**
- `qwen2.5:3b` (Recommended - 2GB, excellent for SQL)
- `phi3:mini` (3.8B parameters)
- `llama3.2:3b` (Meta's model)
- `gemma2:2b` (Lightweight)

**Cloud Models (Recommended):**
- `gemma-3-27b-it` (High Quota / Default)
- `gemini-2.5-flash-lite` (Fast / Efficient)
- `claude-3-5-sonnet` (Anthropic - requires `ANTHROPIC_API_KEY`)

---

## Usage

### Example Queries

**Basic Queries:**
- "Show patient count by state"
- "List all patients with diabetes"
- "Count visits by diagnosis"

**Visualization Queries:**
- "Distribution of patients by gender" ‚Üí Pie Chart
- "Patient count by state on a map" ‚Üí Choropleth Map
- "Show correlation between age and BMI" ‚Üí Scatter Plot
- "Frequency distribution of patient ages" ‚Üí Histogram
- "Show patients by insurance type and income bracket" ‚Üí Sunburst/Treemap

**Advanced Queries:**
- "3D plot of age, BMI, and cholesterol" ‚Üí 3D Scatter
- "Correlation matrix of patient health metrics" ‚Üí Heatmap
- "Patient registrations over time" ‚Üí Line Chart

### Interactive Features

- **Chart Type Switching**: Click any compatible visualization type above the chart
- **Zoom & Pan**: Use Plotly's built-in controls
- **Download**: Export charts as PNG images
- **Chat History**: Conversations persist across sessions (24-hour default)

---



## Local Model Setup (Ollama)

### Option 1: Docker (Included)

Ollama is automatically included in the Docker setup - no separate installation needed!

### Option 2: System Installation

**Windows:**
```powershell
winget install Ollama.Ollama
ollama pull qwen2.5:3b
```

**Ubuntu/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen2.5:3b
```

**Configure:**
```bash
# In backend/.env
USE_LOCAL_MODEL=true
LOCAL_MODEL_NAME=qwen2.5:3b
```

**Full guide:** [backend/docs/LOCAL_MODEL_SETUP.md](backend/docs/LOCAL_MODEL_SETUP.md)

---

## Project Structure

```
medicareAI/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # CSV datasets (enhanced)
‚îÇ   ‚îú‚îÄ‚îÄ services/                # Application logic
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # FastAPI application
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile               # Backend container
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatBox.tsx      # Main chat interface
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PlotlyVisualizer.tsx  # 60+ chart types
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ App.tsx              # Root component
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile               # Frontend container
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml           # Container orchestration (use 'docker compose')
‚îú‚îÄ‚îÄ docker-start.ps1             # Windows quick start
‚îú‚îÄ‚îÄ docker-start.sh              # Linux/Mac quick start
‚îú‚îÄ‚îÄ DOCKER_DEPLOYMENT.md         # Deployment guide
‚îî‚îÄ‚îÄ README.md
```

---

## Supported Visualizations

The system intelligently selects from **60+ Plotly.js chart types**:

| Category | Chart Types |
|----------|-------------|
| **Basic** | Bar, Pie, Donut, Line, Scatter, Area |
| **Statistical** | Box, Violin, Histogram, Heatmap, Contour |
| **Financial** | Waterfall, Funnel, Candlestick, OHLC |
| **3D** | Scatter3D, Surface, Mesh3D |
| **Maps** | Choropleth, ScatterGeo, Mapbox |
| **Hierarchical** | Sunburst, Treemap, Icicle, Sankey |
| **Specialized** | Indicator, Gauge, Parcoords, SPLOM |

**Interactive chart switching** - Click any compatible type to switch views!

---

## Troubleshooting

### Docker Issues

**Ollama model not found:**
```bash
docker exec -it mediquery-ai-ollama ollama pull qwen2.5:3b
```

**Backend can't connect to Ollama:**
```bash
docker compose restart ollama
docker compose restart backend
```

**Port already in use:**
Edit `docker-compose.yml` and change ports (then run `docker compose up -d`):
```yaml
ports:
  - "3001:80"  # Frontend
  - "8001:8000"  # Backend
```

### Manual Installation Issues

**Vite Cache Issues:**
```bash
cd frontend
rm -rf node_modules/.vite
npm run dev
```

**API Key Not Working:**
1. Verify key at https://makersuite.google.com/app/apikey
2. Ensure `.env` is in `backend/` directory
3. Restart backend server

**Chat History Not Persisting:**
Check that `chat_history.db` exists in `backend/` directory.

---

## Development

### Adding New Datasets

1. Add CSV file to `backend/data/`
2. Database service auto-loads all CSV files
3. Restart backend

### Customizing Visualizations

Edit `frontend/src/components/PlotlyVisualizer.tsx`:
- Add new chart types
- Modify color schemes
- Adjust layout configurations

### Hot Reload

- **Backend**: Auto-reloads on code changes (uvicorn --reload)
- **Frontend**: Auto-reloads on code changes (Vite HMR)
- **Docker**: Backend volume-mounted for hot reload

---

## üìà Performance

| Metric | Target | Actual |
|--------|--------|--------|
| **Query Response Time** | < 3s | ‚úÖ 1-3s |
| **Backend Health Check** | < 100ms | ‚úÖ ~50ms |
| **Frontend Load Time** | < 2s | ‚úÖ ~1s |
| **Chart Render Time** | < 500ms | ‚úÖ ~300ms |
| **Concurrent Users** | 10+ | ‚úÖ Tested |

---

## Security Notes

‚ö†Ô∏è **This is a POC/Demo Application**

For production use:
- Implement proper authentication
- Use environment-specific CORS settings
- Add rate limiting
- Sanitize all SQL inputs (currently using parameterized queries)
- Use a production database (PostgreSQL, MySQL)
- Implement HTTPS
- Secure Ollama endpoint
- Use Docker secrets for sensitive data

---

## License

MIT License - See LICENSE file for details

---

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## Acknowledgments

- **Google Gemini AI** for natural language processing
- **Ollama** for local LLM inference
- **Plotly.js** for interactive visualizations
- **FastAPI** for the excellent Python web framework
- **React** and **Vite** for the frontend stack

---

## Support

For issues and questions:
- Open an issue on GitHub
- Check [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md) for Docker help
- Check [LOCAL_MODEL_SETUP.md](backend/docs/LOCAL_MODEL_SETUP.md) for Ollama help

---

**Built with ‚ù§Ô∏è using AI-assisted development**
